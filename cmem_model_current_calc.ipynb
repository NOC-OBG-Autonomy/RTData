{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee49206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from rtdata import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily depth-weighted averages\n",
    "def weighted_avg(da, weights):\n",
    "    weighted_sum = (da * weights).sum(dim='depth')\n",
    "    total_weights = weights.sum(dim='depth')\n",
    "    return weighted_sum / total_weights\n",
    "\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON file with the filenames\n",
    "json_file = os.path.join(config.save_path, 'filenames.json')#\n",
    "\n",
    "# Load the filename from the JSON file\n",
    "with open(json_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Filter for the specific file\n",
    "filename_in_json = None\n",
    "for file in data:\n",
    "    if \"model_currents.nc\" in file:\n",
    "        filename_in_json = file\n",
    "\n",
    "filepath = os.path.join(config.save_path, filename_in_json)\n",
    "filename_in_json = filename_in_json.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dedd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the netCDF file\n",
    "ds = xr.open_dataset(filepath)\n",
    "\n",
    "# Extract the relevant data\n",
    "depth = ds['depth']\n",
    "uo = ds['uo']\n",
    "vo = ds['vo']\n",
    "\n",
    "# Determine the index of the end depth bin (1000m)\n",
    "end_depth_value = 1000  # Assuming 1000m is the end depth bin you are interested in\n",
    "end_depth_index = (depth == end_depth_value).argmax().item()\n",
    "\n",
    "# Extract uo and vo at the end depth bin\n",
    "uo_1000m = uo.isel(depth=end_depth_index)\n",
    "vo_1000m = vo.isel(depth=end_depth_index)\n",
    "\n",
    "# Create a new dataset with the extracted data\n",
    "ds_1000m = xr.Dataset({\n",
    "    'uo': uo_1000m,\n",
    "    'vo': vo_1000m\n",
    "})\n",
    "\n",
    "# Add attributes\n",
    "ds_1000m['uo'].attrs = uo.attrs\n",
    "ds_1000m['vo'].attrs = vo.attrs\n",
    "ds_1000m.attrs = ds.attrs\n",
    "\n",
    "# Calculate depth intervals\n",
    "depth_diff = np.diff(depth, append=depth[-1])\n",
    "\n",
    "# Convert depth_diff to DataArray\n",
    "depth_diff_da = xr.DataArray(depth_diff, coords={'depth': depth}, dims='depth')\n",
    "\n",
    "# Apply the weighted average function for each day\n",
    "weighted_uo = uo.groupby('time').map(weighted_avg, args=(depth_diff_da,))\n",
    "weighted_vo = vo.groupby('time').map(weighted_avg, args=(depth_diff_da,))\n",
    "\n",
    "# Create a new dataset with the averaged data\n",
    "averaged_ds = xr.Dataset({\n",
    "    'uo': weighted_uo,\n",
    "    'vo': weighted_vo\n",
    "})\n",
    "\n",
    "# Add attributes\n",
    "averaged_ds['uo'].attrs = uo.attrs\n",
    "averaged_ds['vo'].attrs = vo.attrs\n",
    "averaged_ds.attrs = ds.attrs\n",
    "\n",
    "# Save the averaged data to a new netCDF file\n",
    "output_file_averaged      = f'{filename_in_json}_averaged.nc'\n",
    "output_file_path_averaged = os.path.join(satellite_dir, output_file_averaged)\n",
    "averaged_ds.to_netcdf(output_file_path_averaged)\n",
    "\n",
    "# Save the dataset to a new NetCDF file\n",
    "output_file_1000m = f'{filename_in_json}_1000m.nc'\n",
    "output_file_path_1000m = os.path.join(satellite_dir, output_file_1000m)\n",
    "ds_1000m.to_netcdf(output_file_path_1000m)\n",
    "\n",
    "# Add new filenames to the JSON file\n",
    "new_filenames = [output_file_averaged, output_file_1000m]\n",
    "filenames.extend(new_filenames)\n",
    "\n",
    "# Add new filenames to the JSON file under \"processed data\" category\n",
    "if 'processed data' not in data:\n",
    "    data['processed data'] = []\n",
    "\n",
    "new_filenames = [output_file_averaged, output_file_1000m]\n",
    "data['processed data'].extend(new_filenames)\n",
    "\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "print(f\"DAC saved to {output_file_path_averaged}\")\n",
    "print(f\"1000m currents saved to {output_file_path_1000m}\")\n",
    "print(f\"Updated filenames saved to {json_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RT_data_wip_hans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
